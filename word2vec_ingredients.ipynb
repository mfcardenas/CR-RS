{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import config\n",
    "from ingredient_parser import ingredient_parser\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "if sys.platform == 'linux':\n",
    "    path = config.LINUX_PATH\n",
    "else:\n",
    "    path = config.OS_PATH\n",
    "os.chdir(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "data = pd.read_csv('input/df_parsed.csv')\n",
    "data['parsed_new'] = data.ingredients.apply(ingredient_parser)\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         recipe_urls  \\\n",
       "0  https://www.jamieoliver.com/recipes/duck-recip...   \n",
       "1  https://www.jamieoliver.com/recipes/vegetable-...   \n",
       "2  https://www.jamieoliver.com/recipes/pasta-reci...   \n",
       "3  https://www.jamieoliver.com/recipes/vegetable-...   \n",
       "4  https://www.jamieoliver.com/recipes/chicken-re...   \n",
       "\n",
       "                     recipe_name  \\\n",
       "0  Roast duck with Marsala gravy   \n",
       "1     Best-ever Brussels sprouts   \n",
       "2  Beautiful courgette carbonara   \n",
       "3     Roasted black bean burgers   \n",
       "4     Chicken & tofu noodle soup   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  ['1 x 1.6kg whole duck', '2 heaped teaspoons C...   \n",
       "1  ['800 g Brussels sprouts', '2 higher-welfare C...   \n",
       "2  ['6 medium green and yellow courgettes', '500 ...   \n",
       "3  ['1½ red onions', '200 g mixed mushrooms', '10...   \n",
       "4  ['2 shallots', '2 cloves of garlic', '2 cm pie...   \n",
       "\n",
       "                                  ingredients_parsed  \\\n",
       "0  duck chinese five clementine gravy carrot onio...   \n",
       "1  brussels sprout cumberland sausage butter onio...   \n",
       "2  courgette penne egg single cream parmesan chee...   \n",
       "3  onion mushroom rye bread bean mature cheddar c...   \n",
       "4  shallot chicken thigh sesame star soy rice noo...   \n",
       "\n",
       "                                          parsed_new  \n",
       "0  [duck, chinese five, clementine, gravy, carrot...  \n",
       "1  [brussels sprout, cumberland sausage, butter, ...  \n",
       "2  [courgette, penne, egg, single cream, parmesan...  \n",
       "3  [onion, mushroom, rye bread, bean, mature ched...  \n",
       "4  [shallot, chicken thigh, sesame, star, soy, ri...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_urls</th>\n",
       "      <th>recipe_name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_parsed</th>\n",
       "      <th>parsed_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.jamieoliver.com/recipes/duck-recip...</td>\n",
       "      <td>Roast duck with Marsala gravy</td>\n",
       "      <td>['1 x 1.6kg whole duck', '2 heaped teaspoons C...</td>\n",
       "      <td>duck chinese five clementine gravy carrot onio...</td>\n",
       "      <td>[duck, chinese five, clementine, gravy, carrot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.jamieoliver.com/recipes/vegetable-...</td>\n",
       "      <td>Best-ever Brussels sprouts</td>\n",
       "      <td>['800 g Brussels sprouts', '2 higher-welfare C...</td>\n",
       "      <td>brussels sprout cumberland sausage butter onio...</td>\n",
       "      <td>[brussels sprout, cumberland sausage, butter, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.jamieoliver.com/recipes/pasta-reci...</td>\n",
       "      <td>Beautiful courgette carbonara</td>\n",
       "      <td>['6 medium green and yellow courgettes', '500 ...</td>\n",
       "      <td>courgette penne egg single cream parmesan chee...</td>\n",
       "      <td>[courgette, penne, egg, single cream, parmesan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.jamieoliver.com/recipes/vegetable-...</td>\n",
       "      <td>Roasted black bean burgers</td>\n",
       "      <td>['1½ red onions', '200 g mixed mushrooms', '10...</td>\n",
       "      <td>onion mushroom rye bread bean mature cheddar c...</td>\n",
       "      <td>[onion, mushroom, rye bread, bean, mature ched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.jamieoliver.com/recipes/chicken-re...</td>\n",
       "      <td>Chicken &amp; tofu noodle soup</td>\n",
       "      <td>['2 shallots', '2 cloves of garlic', '2 cm pie...</td>\n",
       "      <td>shallot chicken thigh sesame star soy rice noo...</td>\n",
       "      <td>[shallot, chicken thigh, sesame, star, soy, ri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# get corpus with the documents sorted in alphabetical order\n",
    "def get_and_sort_corpus(data):\n",
    "    corpus_sorted = []\n",
    "    for doc in data.parsed_new.values:\n",
    "        doc.sort()\n",
    "        corpus_sorted.append(doc)\n",
    "    return corpus_sorted\n",
    "\n",
    "corpus = get_and_sort_corpus(data)\n",
    "print(f\"Length of corpus: {len(corpus)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of corpus: 4647\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# calculate average length of each document \n",
    "lengths = [len(doc) for doc in corpus]\n",
    "avg_len = float(sum(lengths)) / len(lengths)\n",
    "avg_len"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.984506132989025"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# train word2vec model \n",
    "sg = 0 # CBOW: build a language model that correctly predicts the center word given the context words in which the center word appears\n",
    "workers = 8 # number of CPUs\n",
    "window = 6 # window size: average length of each document \n",
    "min_count = 1 # unique ingredients are important to decide recipes \n",
    "\n",
    "model_cbow = Word2Vec(corpus, sg=sg, workers=workers, window=window, min_count=min_count, vector_size=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#Summarize the loaded model\n",
    "print(model_cbow)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_cbow.wv.index_to_key)\n",
    "words.sort()\n",
    "# print(words)\n",
    "\n",
    "#Acess vector for one word\n",
    "# print(model_cbow.wv['chicken stock'])\n",
    "\n",
    "# most similar\n",
    "model_cbow.wv.most_similar(u'cauliflower just larger than potato')\n",
    "model_cbow.wv.similarity('cauliflower', 'cauliflower just larger than potato')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=3893, vector_size=100, alpha=0.025)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.868625"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "model_cbow.save('models/model_cbow.bin')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "\tdef __init__(self, word_model):\n",
    "\t\tself.word_model = word_model\n",
    "\t\tself.vector_size = word_model.wv.vector_size\n",
    "\n",
    "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "\t\tdoc_word_vector = self.word_average_list(docs)\n",
    "\t\treturn doc_word_vector\n",
    "\n",
    "\tdef word_average(self, sent):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for a single doc/sentence.\n",
    "\t\t:param sent: list of sentence tokens\n",
    "\t\t:return:\n",
    "\t\t\tmean: float of averaging word vectors\n",
    "\t\t\"\"\"\n",
    "\t\tmean = []\n",
    "\t\tfor word in sent:\n",
    "\t\t\tif word in self.word_model.wv.index_to_key:\n",
    "\t\t\t\tmean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "\t\tif not mean:  # empty words\n",
    "\t\t\t# If a text is empty, return a vector of zeros.\n",
    "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "\t\t\treturn np.zeros(self.vector_size)\n",
    "\t\telse:\n",
    "\t\t\tmean = np.array(mean).mean(axis=0)\n",
    "\t\t\treturn mean\n",
    "\n",
    "\n",
    "\tdef word_average_list(self, docs):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
    "\t\t:param docs: list of sentence in list of separated tokens\n",
    "\t\t:return:\n",
    "\t\t\tarray of average word vector in shape (len(docs),)\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.vstack([self.word_average(sent) for sent in docs])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# encode document by averaging word embeddings\n",
    "\n",
    "# load model \n",
    "loaded_model = Word2Vec.load('models/model_cbow.bin')\n",
    "if loaded_model:\n",
    "    print(\"Successfully loaded model\")\n",
    "\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(loaded_model)\n",
    "doc_vec = mean_vec_tr.transform(corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully loaded model\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('whatscooking': conda)"
  },
  "interpreter": {
   "hash": "65e86ed0733f1142eec8ce6d200d79e2af223aab50d601078877b07cd7cef66e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}